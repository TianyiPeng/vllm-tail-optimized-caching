{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-24 03:42:39 [__init__.py:248] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# ruff: noqa: E501\n",
    "# A prompt containing a large markdown table. The table is randomly generated by GPT-4.\n",
    "LONG_PROMPT = \"You are a helpful assistant in recognizes the content of tables in markdown format. Here is a table as follows.\\n# Table\\n\" + \"\"\"\n",
    "| ID  | Name          | Age | Occupation    | Country       | Email                  | Phone Number   | Address                       |\n",
    "|-----|---------------|-----|---------------|---------------|------------------------|----------------|------------------------------|\n",
    "| 1   | John Doe      | 29  | Engineer      | USA           | john.doe@example.com   | 555-1234       | 123 Elm St, Springfield, IL  |\n",
    "| 2   | Jane Smith    | 34  | Doctor        | Canada        | jane.smith@example.com | 555-5678       | 456 Oak St, Toronto, ON      |\n",
    "| 3   | Alice Johnson | 27  | Teacher       | UK            | alice.j@example.com    | 555-8765       | 789 Pine St, London, UK      |\n",
    "| 4   | Bob Brown     | 45  | Artist        | Australia     | bob.b@example.com      | 555-4321       | 321 Maple St, Sydney, NSW    |\n",
    "| 5   | Carol White   | 31  | Scientist     | New Zealand   | carol.w@example.com    | 555-6789       | 654 Birch St, Wellington, NZ |\n",
    "| 6   | Dave Green    | 28  | Lawyer        | Ireland       | dave.g@example.com     | 555-3456       | 987 Cedar St, Dublin, IE     |\n",
    "| 7   | Emma Black    | 40  | Musician      | USA           | emma.b@example.com     | 555-1111       | 246 Ash St, New York, NY     |\n",
    "| 8   | Frank Blue    | 37  | Chef          | Canada        | frank.b@example.com    | 555-2222       | 135 Spruce St, Vancouver, BC |\n",
    "| 9   | Grace Yellow  | 50  | Engineer      | UK            | grace.y@example.com    | 555-3333       | 864 Fir St, Manchester, UK   |\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_generation_time(llm, sampling_params, prompts):\n",
    "    # time the generation\n",
    "    start_time = time.time()\n",
    "    output = llm.generate(prompts, sampling_params=sampling_params)\n",
    "    end_time = time.time()\n",
    "    # print the output and generation time\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Output: {output[0].outputs[0].text}\")\n",
    "    print(f\"Generation time: {end_time - start_time} seconds.\")\n",
    "    print(\"-\" * 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-24 03:42:46 [__init__.py:30] Available plugins for group vllm.general_plugins:\n",
      "INFO 05-24 03:42:46 [__init__.py:32] name=lora_filesystem_resolver, value=vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver\n",
      "INFO 05-24 03:42:46 [__init__.py:34] all available plugins for group vllm.general_plugins will be loaded.\n",
      "INFO 05-24 03:42:46 [__init__.py:36] set environment variable VLLM_PLUGINS to control which plugins to load.\n",
      "INFO 05-24 03:42:46 [__init__.py:44] plugin lora_filesystem_resolver loaded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-24 03:43:00 [config.py:788] This model supports multiple tasks: {'embed', 'reward', 'generate', 'score', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 05-24 03:43:00 [config.py:2115] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 05-24 03:43:03 [core.py:438] Waiting for init message from front-end.\n",
      "INFO 05-24 03:43:03 [core.py:65] Initializing a V1 LLM engine (v0.1.dev6658+gaa0760b.d20250524) with config: model='facebook/opt-125m', speculative_config=None, tokenizer='facebook/opt-125m', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=facebook/opt-125m, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\": 3, \"custom_ops\": [\"none\"], \"splitting_ops\": [\"vllm.unified_attention\", \"vllm.unified_attention_with_output\"], \"compile_sizes\": [], \"inductor_compile_config\": {\"enable_auto_functionalized_v2\": false}, \"use_cudagraph\": true, \"cudagraph_num_of_warmups\": 1, \"cudagraph_capture_sizes\": [512, 504, 496, 488, 480, 472, 464, 456, 448, 440, 432, 424, 416, 408, 400, 392, 384, 376, 368, 360, 352, 344, 336, 328, 320, 312, 304, 296, 288, 280, 272, 264, 256, 248, 240, 232, 224, 216, 208, 200, 192, 184, 176, 168, 160, 152, 144, 136, 128, 120, 112, 104, 96, 88, 80, 72, 64, 56, 48, 40, 32, 24, 16, 8, 4, 2, 1], \"max_capture_size\": 512}\n",
      "WARNING 05-24 03:43:03 [utils.py:2670] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x71245e991180>\n",
      "INFO 05-24 03:43:04 [parallel_state.py:1079] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 05-24 03:43:04 [cuda.py:216] Using Flash Attention backend on V1 engine.\n",
      "WARNING 05-24 03:43:04 [topk_topp_sampler.py:58] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 05-24 03:43:04 [gpu_model_runner.py:1515] Starting to load model facebook/opt-125m...\n",
      "INFO 05-24 03:43:04 [backends.py:35] Using InductorAdaptor\n",
      "INFO 05-24 03:43:05 [weight_utils.py:291] Using model weights format ['*.bin']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6dbe051e65d48c6bb6ec97befc5e69f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pt checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-24 03:43:05 [default_loader.py:280] Loading weights took 0.25 seconds\n",
      "INFO 05-24 03:43:05 [gpu_model_runner.py:1533] Model loading took 0.2389 GiB and 1.171501 seconds\n",
      "INFO 05-24 03:43:09 [backends.py:459] Using cache directory: /home/ubuntu/.cache/vllm/torch_compile_cache/ce3858e353/rank_0_0 for vLLM's torch.compile\n",
      "INFO 05-24 03:43:09 [backends.py:469] Dynamo bytecode transform time: 3.73 s\n",
      "INFO 05-24 03:43:11 [backends.py:132] Directly load the compiled graph(s) for shape None from the cache, took 1.220 s\n",
      "INFO 05-24 03:43:11 [monitor.py:33] torch.compile takes 3.73 s in total\n",
      "INFO 05-24 03:43:12 [kv_cache_utils.py:639] GPU KV cache size: 274,080 tokens\n",
      "INFO 05-24 03:43:12 [kv_cache_utils.py:642] Maximum concurrency for 2,048 tokens per request: 133.83x\n",
      "INFO 05-24 03:43:28 [gpu_model_runner.py:1909] Graph capturing finished in 16 secs, took 0.20 GiB\n",
      "INFO 05-24 03:43:28 [core.py:167] init engine (profile, create kv cache, warmup model) took 22.08 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "llm = LLM(\n",
    "    model=\"facebook/opt-125m\",\n",
    "    enable_prefix_caching=True,\n",
    "    caching_low_priority_last_num_tokens=32  # You can adjust this value based on your needs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vllm_config': VllmConfig(model_config=ModelConfig(model='facebook/opt-125m', task='generate', tokenizer='facebook/opt-125m', tokenizer_mode='auto', trust_remote_code=False, dtype=torch.float16, seed=0, hf_config_path=None, allowed_local_media_path='', revision=None, code_revision=None, rope_scaling={}, rope_theta=None, tokenizer_revision=None, max_model_len=2048, spec_target_max_model_len=None, quantization=None, enforce_eager=False, max_seq_len_to_capture=2048, max_logprobs=20, disable_sliding_window=False, disable_cascade_attn=False, skip_tokenizer_init=False, enable_prompt_embeds=False, served_model_name='facebook/opt-125m', limit_mm_per_prompt={}, use_async_output_proc=True, config_format=<ConfigFormat.AUTO: 'auto'>, hf_token=None, hf_overrides=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, override_neuron_config={}, pooler_config=None, override_pooler_config=None, logits_processor_pattern=None, generation_config='auto', override_generation_config={}, enable_sleep_mode=False, model_impl='auto'), cache_config=CacheConfig(block_size=16, gpu_memory_utilization=0.9, swap_space=4, cache_dtype='auto', is_attention_free=False, num_gpu_blocks_override=None, sliding_window=None, enable_prefix_caching=True, caching_low_priority_last_num_tokens=32, prefix_caching_hash_algo='builtin', cpu_offload_gb=0, calculate_kv_scales=False, num_gpu_blocks=17130, num_cpu_blocks=None), parallel_config=ParallelConfig(pipeline_parallel_size=1, tensor_parallel_size=1, data_parallel_size=1, data_parallel_size_local=1, data_parallel_rank=0, data_parallel_rank_local=0, data_parallel_master_ip='127.0.0.1', data_parallel_rpc_port=29550, data_parallel_master_port=0, enable_expert_parallel=False, max_parallel_loading_workers=None, disable_custom_all_reduce=False, tokenizer_pool_config=None, ray_workers_use_nsight=False, placement_group=None, distributed_executor_backend='uni', worker_cls='vllm.v1.worker.gpu_worker.Worker', sd_worker_cls='auto', worker_extension_cls='', world_size=1, rank=0), scheduler_config=SchedulerConfig(runner_type='generate', max_num_batched_tokens=8192, max_num_seqs=256, max_model_len=2048, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, num_lookahead_slots=0, cuda_graph_sizes=[512], delay_factor=0.0, enable_chunked_prefill=True, is_multimodal_model=False, max_num_encoder_input_tokens=8192, encoder_cache_size=8192, preemption_mode=None, num_scheduler_steps=1, multi_step_stream_outputs=True, send_delta_data=False, policy='fcfs', chunked_prefill_enabled=True, disable_chunked_mm_input=False, scheduler_cls='vllm.v1.core.sched.scheduler.Scheduler'), device_config=DeviceConfig(device=device(type='cuda'), device_type='cuda'), load_config=LoadConfig(load_format=<LoadFormat.AUTO: 'auto'>, download_dir=None, model_loader_extra_config={}, ignore_patterns=['original/**/*'], use_tqdm_on_load=True, pt_load_map_location='cpu'), lora_config=None, speculative_config=None, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), prompt_adapter_config=None, quant_config=None, compilation_config={\"level\": 3, \"custom_ops\": [\"none\"], \"splitting_ops\": [\"vllm.unified_attention\", \"vllm.unified_attention_with_output\"], \"compile_sizes\": [], \"inductor_compile_config\": {\"enable_auto_functionalized_v2\": false}, \"use_cudagraph\": true, \"cudagraph_num_of_warmups\": 1, \"cudagraph_capture_sizes\": [512, 504, 496, 488, 480, 472, 464, 456, 448, 440, 432, 424, 416, 408, 400, 392, 384, 376, 368, 360, 352, 344, 336, 328, 320, 312, 304, 296, 288, 280, 272, 264, 256, 248, 240, 232, 224, 216, 208, 200, 192, 184, 176, 168, 160, 152, 144, 136, 128, 120, 112, 104, 96, 88, 80, 72, 64, 56, 48, 40, 32, 24, 16, 8, 4, 2, 1], \"max_capture_size\": 512}, kv_transfer_config=None, kv_events_config=None, additional_config=None, instance_id='dac5b'),\n",
       " 'model_config': ModelConfig(model='facebook/opt-125m', task='generate', tokenizer='facebook/opt-125m', tokenizer_mode='auto', trust_remote_code=False, dtype=torch.float16, seed=0, hf_config_path=None, allowed_local_media_path='', revision=None, code_revision=None, rope_scaling={}, rope_theta=None, tokenizer_revision=None, max_model_len=2048, spec_target_max_model_len=None, quantization=None, enforce_eager=False, max_seq_len_to_capture=2048, max_logprobs=20, disable_sliding_window=False, disable_cascade_attn=False, skip_tokenizer_init=False, enable_prompt_embeds=False, served_model_name='facebook/opt-125m', limit_mm_per_prompt={}, use_async_output_proc=True, config_format=<ConfigFormat.AUTO: 'auto'>, hf_token=None, hf_overrides=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, override_neuron_config={}, pooler_config=None, override_pooler_config=None, logits_processor_pattern=None, generation_config='auto', override_generation_config={}, enable_sleep_mode=False, model_impl='auto'),\n",
       " 'cache_config': CacheConfig(block_size=16, gpu_memory_utilization=0.9, swap_space=4, cache_dtype='auto', is_attention_free=False, num_gpu_blocks_override=None, sliding_window=None, enable_prefix_caching=True, caching_low_priority_last_num_tokens=32, prefix_caching_hash_algo='builtin', cpu_offload_gb=0, calculate_kv_scales=False, num_gpu_blocks=17130, num_cpu_blocks=None),\n",
       " 'dp_group': None,\n",
       " 'should_execute_dummy_batch': False,\n",
       " 'tokenizer': <vllm.transformers_utils.tokenizer_group.TokenizerGroup at 0x712466307f10>,\n",
       " 'processor': <vllm.v1.engine.processor.Processor at 0x712466305ba0>,\n",
       " 'output_processor': <vllm.v1.engine.output_processor.OutputProcessor at 0x71245ed3ca90>,\n",
       " 'engine_core': <vllm.v1.engine.core_client.SyncMPClient at 0x71245e990280>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.llm_engine.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vllm_config': VllmConfig(model_config=ModelConfig(model='facebook/opt-125m', task='generate', tokenizer='facebook/opt-125m', tokenizer_mode='auto', trust_remote_code=False, dtype=torch.float16, seed=0, hf_config_path=None, allowed_local_media_path='', revision=None, code_revision=None, rope_scaling={}, rope_theta=None, tokenizer_revision=None, max_model_len=2048, spec_target_max_model_len=None, quantization=None, enforce_eager=False, max_seq_len_to_capture=2048, max_logprobs=20, disable_sliding_window=False, disable_cascade_attn=False, skip_tokenizer_init=False, enable_prompt_embeds=False, served_model_name='facebook/opt-125m', limit_mm_per_prompt={}, use_async_output_proc=True, config_format=<ConfigFormat.AUTO: 'auto'>, hf_token=None, hf_overrides=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, override_neuron_config={}, pooler_config=None, override_pooler_config=None, logits_processor_pattern=None, generation_config='auto', override_generation_config={}, enable_sleep_mode=False, model_impl='auto'), cache_config=CacheConfig(block_size=16, gpu_memory_utilization=0.9, swap_space=4, cache_dtype='auto', is_attention_free=False, num_gpu_blocks_override=None, sliding_window=None, enable_prefix_caching=True, caching_low_priority_last_num_tokens=32, prefix_caching_hash_algo='builtin', cpu_offload_gb=0, calculate_kv_scales=False, num_gpu_blocks=17130, num_cpu_blocks=None), parallel_config=ParallelConfig(pipeline_parallel_size=1, tensor_parallel_size=1, data_parallel_size=1, data_parallel_size_local=1, data_parallel_rank=0, data_parallel_rank_local=0, data_parallel_master_ip='127.0.0.1', data_parallel_rpc_port=29550, data_parallel_master_port=0, enable_expert_parallel=False, max_parallel_loading_workers=None, disable_custom_all_reduce=False, tokenizer_pool_config=None, ray_workers_use_nsight=False, placement_group=None, distributed_executor_backend='uni', worker_cls='vllm.v1.worker.gpu_worker.Worker', sd_worker_cls='auto', worker_extension_cls='', world_size=1, rank=0), scheduler_config=SchedulerConfig(runner_type='generate', max_num_batched_tokens=8192, max_num_seqs=256, max_model_len=2048, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, num_lookahead_slots=0, cuda_graph_sizes=[512], delay_factor=0.0, enable_chunked_prefill=True, is_multimodal_model=False, max_num_encoder_input_tokens=8192, encoder_cache_size=8192, preemption_mode=None, num_scheduler_steps=1, multi_step_stream_outputs=True, send_delta_data=False, policy='fcfs', chunked_prefill_enabled=True, disable_chunked_mm_input=False, scheduler_cls='vllm.v1.core.sched.scheduler.Scheduler'), device_config=DeviceConfig(device=device(type='cuda'), device_type='cuda'), load_config=LoadConfig(load_format=<LoadFormat.AUTO: 'auto'>, download_dir=None, model_loader_extra_config={}, ignore_patterns=['original/**/*'], use_tqdm_on_load=True, pt_load_map_location='cpu'), lora_config=None, speculative_config=None, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), prompt_adapter_config=None, quant_config=None, compilation_config={\"level\": 3, \"custom_ops\": [\"none\"], \"splitting_ops\": [\"vllm.unified_attention\", \"vllm.unified_attention_with_output\"], \"compile_sizes\": [], \"inductor_compile_config\": {\"enable_auto_functionalized_v2\": false}, \"use_cudagraph\": true, \"cudagraph_num_of_warmups\": 1, \"cudagraph_capture_sizes\": [512, 504, 496, 488, 480, 472, 464, 456, 448, 440, 432, 424, 416, 408, 400, 392, 384, 376, 368, 360, 352, 344, 336, 328, 320, 312, 304, 296, 288, 280, 272, 264, 256, 248, 240, 232, 224, 216, 208, 200, 192, 184, 176, 168, 160, 152, 144, 136, 128, 120, 112, 104, 96, 88, 80, 72, 64, 56, 48, 40, 32, 24, 16, 8, 4, 2, 1], \"max_capture_size\": 512}, kv_transfer_config=None, kv_events_config=None, additional_config=None, instance_id='dac5b'),\n",
       " 'model_config': ModelConfig(model='facebook/opt-125m', task='generate', tokenizer='facebook/opt-125m', tokenizer_mode='auto', trust_remote_code=False, dtype=torch.float16, seed=0, hf_config_path=None, allowed_local_media_path='', revision=None, code_revision=None, rope_scaling={}, rope_theta=None, tokenizer_revision=None, max_model_len=2048, spec_target_max_model_len=None, quantization=None, enforce_eager=False, max_seq_len_to_capture=2048, max_logprobs=20, disable_sliding_window=False, disable_cascade_attn=False, skip_tokenizer_init=False, enable_prompt_embeds=False, served_model_name='facebook/opt-125m', limit_mm_per_prompt={}, use_async_output_proc=True, config_format=<ConfigFormat.AUTO: 'auto'>, hf_token=None, hf_overrides=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, override_neuron_config={}, pooler_config=None, override_pooler_config=None, logits_processor_pattern=None, generation_config='auto', override_generation_config={}, enable_sleep_mode=False, model_impl='auto'),\n",
       " 'cache_config': CacheConfig(block_size=16, gpu_memory_utilization=0.9, swap_space=4, cache_dtype='auto', is_attention_free=False, num_gpu_blocks_override=None, sliding_window=None, enable_prefix_caching=True, caching_low_priority_last_num_tokens=32, prefix_caching_hash_algo='builtin', cpu_offload_gb=0, calculate_kv_scales=False, num_gpu_blocks=17130, num_cpu_blocks=None),\n",
       " 'lora_config': None,\n",
       " 'decoding_config': DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''),\n",
       " 'tokenizer': <vllm.transformers_utils.tokenizer_group.TokenizerGroup at 0x712466307f10>,\n",
       " 'generation_config_fields': {'pad_token_id': 1,\n",
       "  'bos_token_id': 2,\n",
       "  'eos_token_id': 2,\n",
       "  '_from_model_config': True,\n",
       "  'transformers_version': '4.52.3'},\n",
       " 'input_preprocessor': <vllm.inputs.preprocess.InputPreprocessor at 0x71245ed3f0d0>,\n",
       " 'mm_input_cache_client': <vllm.v1.engine.mm_input_cache.MirroredProcessingCache at 0x71245e990040>,\n",
       " 'use_hash': True}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.llm_engine.processor.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd3c5f24511042a49e7a953125827332",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a59391cfa404b5ba81968fb075c266d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Output:                                                                                                     \n",
      "Generation time: 0.23444461822509766 seconds.\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d853dddbb1cf4600af95b0c9d23b78c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "012d40a58f5d4d43ae95aa7a6e61bc2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Output:                                                                                                     \n",
      "Generation time: 0.23520183563232422 seconds.\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "sampling_params = SamplingParams(temperature=0, max_tokens=100)\n",
    "\n",
    "# Querying the age of John Doe\n",
    "get_generation_time(\n",
    "    llm,\n",
    "    sampling_params,\n",
    "    LONG_PROMPT +\n",
    "    \"Question: what is the age of John Doe? Your answer: The age of John Doe is \",\n",
    ")\n",
    "\n",
    "# Querying the age of Zack Blue\n",
    "# This query will be faster since vllm avoids computing the KV cache of LONG_PROMPT again.\n",
    "get_generation_time(\n",
    "    llm,\n",
    "    sampling_params,\n",
    "    LONG_PROMPT +\n",
    "    \"Question: what is the age of Zack Blue? Your answer: The age of Zack Blue is \",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
